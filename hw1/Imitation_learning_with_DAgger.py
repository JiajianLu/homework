import tensorflow as tf
import numpy as np
import pickle
import tf_util
import gym
import load_policy

def load_rollout(filename):
    """
    Extract data related to rollout generated by expert policy.
    :param filename - str, name of file.
    :return data - A tuple of lists of observations and actions.
    """
    with open(filename, 'rb') as f:
        data = pickle.loads(f.read())
    
    return data['observations'], data['actions']



def tf_reset():
    try:
        sess.close()
    except:
        pass
    tf.reset_default_graph()
    return tf.Session()

def create_model(num_input_neuron, num_output_neuron):
    # create inputs
    input_ph = tf.placeholder(dtype=tf.float32, shape=[None, num_input_neuron])
    output_ph = tf.placeholder(dtype=tf.float32, shape=[None, num_output_neuron])

    # create variables
    W0 = tf.get_variable(name='W0', shape=[num_input_neuron, 128], initializer=tf.contrib.layers.xavier_initializer()) 
    W1 = tf.get_variable(name='W1', shape=[128, 128], initializer=tf.contrib.layers.xavier_initializer())
    W2 = tf.get_variable(name='W2', shape=[128, 128], initializer=tf.contrib.layers.xavier_initializer())
    W3 = tf.get_variable(name='W3', shape=[128, num_output_neuron], initializer=tf.contrib.layers.xavier_initializer()) 

    b0 = tf.get_variable(name='b0', shape=[128], initializer=tf.constant_initializer(0.))
    b1 = tf.get_variable(name='b1', shape=[128], initializer=tf.constant_initializer(0.))
    b2 = tf.get_variable(name='b2', shape=[128], initializer=tf.constant_initializer(0.))
    b3 = tf.get_variable(name='b3', shape=[num_output_neuron], initializer=tf.constant_initializer(0.))

    weights = [W0, W1, W2, W3]
    biases = [b0, b1, b2, b3]
    activations = [tf.nn.relu, tf.nn.relu, tf.nn.relu, None]

    # create computation graph
    layer = input_ph
    for W, b, activation in zip(weights, biases, activations):
        layer = tf.matmul(layer, W) + b
        if activation is not None:
            layer = activation(layer)
    output_pred = layer
    
    return input_ph, output_ph, output_pred


'''
python Imitation_learning_with_DAgger.py experts/Hopper-v2.pkl expert_data/Hopper-v2.pkl Hopper-v2 --render     
        --num_rollouts 20
'''
def main():
	import argparse
	parser = argparse.ArgumentParser()
	parser.add_argument('expert_policy_file', type=str)
	parser.add_argument('expert_data_file', type=str)
	parser.add_argument('envname', type=str)
	parser.add_argument('--render', action='store_true')
	parser.add_argument("--max_timesteps", type=int)
	parser.add_argument('--num_rollouts', type=int, default=20,
				help='Number of expert roll outs')
	args = parser.parse_args()
	
	filename = args.expert_data_file
	data = load_rollout(filename)
	x = data[0]
	y = []
	for i in range(len(x)):
		y.append(data[1][i][0])
	y = np.array(y)
	num_input_neuron = np.shape(x)[1]
	num_output_neuron = np.shape(y)[1]
	
	#create model
	input_ph, output_ph, output_pred = create_model(num_input_neuron, num_output_neuron)
	    
	# create loss
	mse = tf.reduce_mean(0.5 * tf.square(output_pred - output_ph))

	# create optimizer
	opt = tf.train.AdamOptimizer().minimize(mse)

	saver = tf.train.Saver()
	env = gym.make(args.envname)

	#DAgger loop: Aggregate data and train the model 
	dagger_mean = []
	dagger_var = []
	for k in range(5):

		with tf.Session() as sess:
			#initialize variables
			sess.run(tf.global_variables_initializer())
			tf_util.initialize()
			# run training
			batch_size = 100 #every time pass 100 rows of data

			epochs = 10 #training epoch is 10

			for epoch in range(epochs):
				for training_step in range(len(x)//batch_size):
				    # get a random subset of the training data
					indices = np.random.randint(low=0, high=len(x), size=batch_size)
					input_batch = x[indices]
					output_batch = y[indices]
				    
				    # run the optimizer and get the mse
					_, mse_run = sess.run([opt, mse], feed_dict={input_ph: input_batch, output_ph: output_batch})
				    
				    # print the mse every so often
				print('{0:04d} mse: {1:.3f}'.format(epoch, mse_run))
				saver.save(sess, '/tmp/model.ckpt')

			max_steps = args.max_timesteps or env.spec.timestep_limit
			policy_fn = load_policy.load_policy(args.expert_policy_file)
			returns = []
			observations = []
			actions = []
			for i in range(args.num_rollouts):
				print('iter', i)
				obs = env.reset()
				done = False
				totalr = 0.
				steps = 0
				while not done:
					#get expert's action
					expert_action = policy_fn(obs[None,:]) 

					#append (observations, expert's actions) to the training set
					x = np.append(x,obs[None,:],axis = 0)  
					y = np.append(y,expert_action, axis = 0)

					#our model's action
					action = sess.run(output_pred, feed_dict={input_ph: obs[None, :]})
					observations.append(obs)
					actions.append(action)
					obs, r, done, _ = env.step(action) #r is reward
					totalr += r #total reward
					steps += 1
					if args.render:
						env.render()
					if steps % 100 == 0: 
						print("%i/%i"%(steps, max_steps))
					if steps >= max_steps:
						break
				returns.append(totalr)

			print('returns', returns)
			print('mean return', np.mean(returns))
			dagger_mean.append(np.mean(returns))
			print('std of return', np.std(returns))
			dagger_var.append(np.std(returns))

	from matplotlib.pyplot import text
	import matplotlib
	from matplotlib import pyplot as plt
	expert = 3779.322590705575
	BC_Agent = 1783.6471726576492
	xaxis = [i for i in range(5)]
	fig, ax = plt.subplots()
	ax.errorbar(xaxis, dagger_mean, yerr=dagger_var, uplims=True, lolims = True, ecolor='c')
	ax.hlines(y=BC_Agent, xmin=0, xmax=4, linewidth=2, color='r')
	ax.hlines(y=expert, xmin=0, xmax=4, linewidth=2, color='g')
	ax.set_xticks([0,1,2,3,4])
	ax.set_xlabel('Number of DAgger Interations')
	ax.set_ylabel('Mean of Returns')
	ax.set_title("BC Agent's Performance improved by DAgger")
	text(3.5,BC_Agent+100, "BC Agent")
	text(0,expert-200, "Expert")
	plt.show()


if __name__ == '__main__':
	main()

