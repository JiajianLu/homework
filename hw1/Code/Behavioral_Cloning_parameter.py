import tensorflow as tf
import numpy as np
import pickle
import tf_util
import load_policy

def load_rollout(filename):
    """
    Extract data related to rollout generated by expert policy.
    :param filename - str, name of file.
    :return data - A tuple of lists of observations and actions.
    """
    with open(filename, 'rb') as f:
        data = pickle.loads(f.read())
    
    return data['observations'], data['actions']



def tf_reset():
    try:
        sess.close()
    except:
        pass
    tf.reset_default_graph()
    return tf.Session()


'''
python Behavioral_Cloning_parameter.py expert_data/Humanoid-v2.pkl Humanoid-v2 --render \
            --num_rollouts 20
'''
def main():
	import argparse
	parser = argparse.ArgumentParser()
	parser.add_argument('expert_data_file', type=str)
	parser.add_argument('envname', type=str)
	parser.add_argument('--render', action='store_true')
	parser.add_argument("--max_timesteps", type=int)
	parser.add_argument('--num_rollouts', type=int, default=20,
				help='Number of expert roll outs')
	args = parser.parse_args()
	filename = args.expert_data_file
	data = load_rollout(filename)
	x = data[0]
	y = []
	for i in range(len(x)):
		y.append(data[1][i][0])
	y = np.array(y)
	num_input_neuron = np.shape(x)[1]
	num_output_neuron = np.shape(y)[1]
	def create_model():
	    # create inputs
	    input_ph = tf.placeholder(dtype=tf.float32, shape=[None, num_input_neuron])
	    output_ph = tf.placeholder(dtype=tf.float32, shape=[None, num_output_neuron])

	    # create variables
	    W0 = tf.get_variable(name='W0', shape=[num_input_neuron, 128], initializer=tf.contrib.layers.xavier_initializer()) 
	    W1 = tf.get_variable(name='W1', shape=[128, 128], initializer=tf.contrib.layers.xavier_initializer())
	    W2 = tf.get_variable(name='W2', shape=[128, 128], initializer=tf.contrib.layers.xavier_initializer())
	    W3 = tf.get_variable(name='W3', shape=[128, num_output_neuron], initializer=tf.contrib.layers.xavier_initializer()) 

	    b0 = tf.get_variable(name='b0', shape=[128], initializer=tf.constant_initializer(0.))
	    b1 = tf.get_variable(name='b1', shape=[128], initializer=tf.constant_initializer(0.))
	    b2 = tf.get_variable(name='b2', shape=[128], initializer=tf.constant_initializer(0.))
	    b3 = tf.get_variable(name='b3', shape=[num_output_neuron], initializer=tf.constant_initializer(0.))

	    weights = [W0, W1, W2, W3]
	    biases = [b0, b1, b2, b3]
	    activations = [tf.nn.relu, tf.nn.relu, tf.nn.relu, None]

	    # create computation graph
	    layer = input_ph
	    for W, b, activation in zip(weights, biases, activations):
	        layer = tf.matmul(layer, W) + b
	        if activation is not None:
	            layer = activation(layer)
	    output_pred = layer
	    
	    return input_ph, output_ph, output_pred

	input_ph, output_ph, output_pred = create_model()
	    
	# create loss
	mse = tf.reduce_mean(0.5 * tf.square(output_pred - output_ph))

	# create optimizer
	opt = tf.train.AdamOptimizer().minimize(mse)

	# initialize variables
	
	# create saver to save model variables
	#saver = tf.train.Saver()
	mean = []
	# run training
	batch_size = 100 #every time pass 100 rows of data

	# train the entire data set for 200 times
	for epochs in range(20,250,30):
	#epochs = 200
		with tf.Session() as sess:
			sess.run(tf.global_variables_initializer())
			for epoch in range(epochs):
				for training_step in range(len(x)//batch_size):
				    # get a random subset of the training data
					indices = np.random.randint(low=0, high=len(x), size=batch_size)
					input_batch = x[indices]
					output_batch = y[indices]
				    
				    # run the optimizer and get the mse
					_, mse_run = sess.run([opt, mse], feed_dict={input_ph: input_batch, output_ph: output_batch})
				    
				    # print the mse every so often
				print('{0:04d} mse: {1:.3f}'.format(epoch, mse_run))

			import gym
			env = gym.make(args.envname)
			max_steps = args.max_timesteps or env.spec.timestep_limit

			returns = []
			observations = []
			actions = []
			for i in range(args.num_rollouts):
			    print('iter', i)
			    obs = env.reset()

			    done = False
			    totalr = 0.
			    steps = 0
			    while not done:
			        action = sess.run(output_pred, feed_dict={input_ph: obs[None, :]})
			        observations.append(obs)
			        actions.append(action)
			        obs, r, done, _ = env.step(action) #r is reward
			        totalr += r #total reward
			        steps += 1
			        if args.render:
			            env.render()
			        if steps % 100 == 0: 
			            print("%i/%i"%(steps, max_steps))
			        if steps >= max_steps:
			            break
			    returns.append(totalr)

			print('returns', returns)
			print('mean return', np.mean(returns))
			mean.append(np.mean(returns))
			print('std of return', np.std(returns))

	from matplotlib import pyplot as plt
	import matplotlib
	#plot mean of return vs num of training epochs
	plt.plot([i for i in range(20,250,30)], mean)
	plt.ylabel('Mean of Returns')
	plt.xlabel('Number of Training Epochs')
	plt.title("BC agent's Performance versus Training Epochs")
	plt.show()

if __name__ == '__main__':
	main()

